{"nbformat": 4, "nbformat_minor": 5, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.11.8"}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Runpod: HF → Canary (NeMo) — PyTorch 2.8 / CUDA 12.8\n\nБлокнот для Runpod / PyTorch 2.8 / CUDA 12.8. Без Parquet: экспорт WAV 16 kHz mono + JSONL. Исправлен импорт NeMo: перед установкой/импортом nemo_toolkit[asr] выравниваем NumPy/SciPy/Lightning/TorchMetrics.\n"}, {"cell_type": "markdown", "metadata": {}, "source": "# Проверка окружения\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "import os, sys, subprocess, platform\nimport torch\n\nprint(\"Python :\", sys.version)\nprint(\"Platform:\", platform.platform())\nprint(\"Torch  :\", torch.__version__, \"| CUDA:\", torch.version.cuda)\nprint(\"CUDA visible:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\ntry:\n    subprocess.run([\"nvidia-smi\"], check=True)\nexcept Exception as e:\n    print(\"[WARN] nvidia-smi not available:\", e)\n"}, {"cell_type": "markdown", "metadata": {}, "source": "# Конфиг\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "from pathlib import Path\nimport os, re\n\n# --- Ключ HF: вставь в переменные окружения контейнера ---\n# os.environ[\"HF_TOKEN\"] = \"hf_XXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n# os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = os.environ.get(\"HF_TOKEN\",\"\")\n\nROOT = Path.cwd()\nDATA_DIR    = ROOT / \"data_wav\"\nOUT_DIR     = ROOT / \"filtered_datasets\"\nTMP_DIR     = ROOT / \".tmp\"\nHF_HOME_DIR = ROOT / \".hf\"\n\nfor d in [DATA_DIR, OUT_DIR, TMP_DIR, HF_HOME_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\nos.environ.setdefault(\"HF_HOME\", str(HF_HOME_DIR))\nos.environ.setdefault(\"HF_HUB_CACHE\", str(HF_HOME_DIR / \"hub\"))\nos.environ.setdefault(\"TRANSFORMERS_CACHE\", str(HF_HOME_DIR / \"transformers\"))\nos.environ.setdefault(\"TMP\", str(TMP_DIR))\nos.environ.setdefault(\"TEMP\", str(TMP_DIR))\n\nMODEL_ID    = \"nvidia/canary-1b-v2\"\nNEMO_PATH   = None\nSOURCE_LANG = \"ru\"\nTARGET_LANG = \"ru\"\nTASK        = \"asr\"\nUSE_PNC     = True\nBATCH_SIZE  = 16\n\nMIN_DUR, MAX_DUR = 1.0, 35.0\nCER_MAX, WER_MAX = 0.15, 0.50\nQ_CORE, Q_HARD   = 0.60, 0.95\n\nLINKS = [\n    \"bond005/taiga_speech_v2\",\n    \"bond005/rulibrispeech\",\n    \"bond005/podlodka_speech\",\n    \"bond005/audioset-nonspeech\",\n    \"mozilla-foundation/common_voice_17_0\",\n    \"google/fleurs\",\n]\n\nSPLITS = [\"train\", \"validation\", \"test\"]\nRUPAT  = re.compile(r\"(^|[-_])ru([-_]|$)|russian\", re.IGNORECASE)\n"}, {"cell_type": "markdown", "metadata": {}, "source": "# Установка базовых зависимостей (без переустановки torch)\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "import sys, subprocess\n\ndef pip_install(args):\n    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-U\", *args]\n    print(\"+\", \" \".join(cmd))\n    subprocess.run(cmd, check=True)\n\nbase = [\n    \"datasets[audio]>=2.20.0\",\n    \"huggingface_hub>=0.24\",\n    \"soundfile>=0.12\",\n    \"pydub>=0.25\",\n    \"jiwer>=3.0.0\",\n    \"pandas>=2.2.0\",\n    \"tqdm>=4.66\"\n]\npip_install(base)\n\nprint(\"OK: базовые пакеты установлены\")\n"}, {"cell_type": "markdown", "metadata": {}, "source": "# FIX: стек NumPy/SciPy/Lightning/TorchMetrics + установка NeMo\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "import sys, subprocess\n\ndef run(cmd):\n    print(\"+\", \" \".join(cmd))\n    subprocess.run(cmd, check=True)\n\nrun([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"pip>=24.2\", \"setuptools>=75\", \"wheel>=0.44\"])\n\nrun([sys.executable, \"-m\", \"pip\", \"install\", \"-U\",\n     \"numpy>=2.2,<2.4\",\n     \"scipy>=1.14,<1.17\",\n     \"torchmetrics>=1.5.2\",\n     \"lightning>=2.5.2,<2.6\"\n])\n\n# Install NeMo if missing\ntry:\n    from nemo.collections.asr.models import ASRModel\n    import nemo\n    print(\"NeMo already present:\", nemo.__version__)\nexcept Exception:\n    run([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"nemo_toolkit[asr]==2.4.0\"])\n    from nemo.collections.asr.models import ASRModel\n    import nemo\n\nimport numpy, scipy, lightning, torchmetrics, nemo as _nemo\nprint(\"NumPy      :\", numpy.__version__)\nprint(\"SciPy      :\", scipy.__version__)\nprint(\"Lightning  :\", lightning.__version__)\nprint(\"TorchMetrics:\", torchmetrics.__version__)\nprint(\"NeMo       :\", _nemo.__version__)\n\n# shim for `import numpy.char` if some dep expects it\nimport types, sys as _sys\nif \"numpy.char\" not in _sys.modules:\n    import numpy as _np\n    _mod = types.ModuleType(\"numpy.char\")\n    for k in dir(_np.char):\n        setattr(_mod, k, getattr(_np.char, k))\n    _sys.modules[\"numpy.char\"] = _mod\n    print(\"[shim] injected numpy.char module\")\n\nprint(\"OK: NeMo стек готов\")\n"}, {"cell_type": "markdown", "metadata": {}, "source": "# Хелперы: WAV + JSONL\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "import os, io, json, hashlib\nfrom pathlib import Path\nimport numpy as np\nimport soundfile as sf\nfrom datasets import load_dataset, Audio\nfrom tqdm import tqdm\n\ndef sha1_name(s: str) -> str:\n    return hashlib.sha1(s.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n\ndef ensure_wav_mono16k(data, sr):\n    # returns np.float32 mono at 16k sampling rate\n    import numpy as np\n    if getattr(data, \"ndim\", 1) > 1:\n        data = data.mean(axis=1)\n    data = np.asarray(data, dtype=np.float32)\n    if sr != 16000:\n        try:\n            import librosa\n            data = librosa.resample(y=data, orig_sr=sr, target_sr=16000)\n            sr = 16000\n        except Exception:\n            raise RuntimeError(\"Need resample to 16k but librosa not available\")\n    return data, 16000\n\ndef save_wav(path: Path, data, sr: int):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    sf.write(str(path), data, sr, subtype=\"PCM_16\", format=\"WAV\")\n\ndef prepare_hf_dataset_to_wav(repo: str, split: str, out_root: Path, lang_regex, hf_token=None):\n    kwargs = {}\n    if hf_token:\n        kwargs[\"token\"] = hf_token\n    try:\n        ds = load_dataset(repo, split=split, streaming=False, **kwargs)\n    except Exception as e:\n        print(f\"[skip] {repo}:{split} → {e}\")\n        return None\n\n    try:\n        ds = ds.cast_column(\"audio\", Audio(sampling_rate=16000))\n    except Exception:\n        pass\n\n    name = f\"{repo.replace('/','___')}_{split}\"\n    out_dir = out_root / name\n    audio_dir = out_dir / \"audio\"\n    out_dir.mkdir(parents=True, exist_ok=True)\n    manifest = out_dir / \"manifest.jsonl\"\n\n    kept = 0\n    with manifest.open(\"w\", encoding=\"utf-8\") as fo:\n        for i, row in enumerate(tqdm(ds, desc=f\"{repo}:{split}\")):\n            text = None\n            for key in [\"text\",\"sentence\",\"transcript\",\"transcription\",\"label\",\"target\"]:\n                if key in row and row[key]:\n                    text = str(row[key]).strip()\n                    break\n            if not text:\n                continue\n\n            lang_val = None\n            for lkey in [\"lang\",\"language\",\"source_lang\",\"locale\"]:\n                if lkey in row and row[lkey]:\n                    lang_val = str(row[lkey]).lower()\n                    break\n            if lang_val and not lang_regex.search(lang_val):\n                continue\n\n            audio = row.get(\"audio\")\n            if not audio:\n                continue\n\n            arr = audio[\"array\"]\n            sr  = audio[\"sampling_rate\"]\n            try:\n                arr, sr = ensure_wav_mono16k(np.asarray(arr), int(sr))\n            except Exception:\n                continue\n\n            dur = float(len(arr) / sr)\n            if not (MIN_DUR <= dur <= MAX_DUR):\n                continue\n\n            wav_path = audio_dir / f\"{sha1_name(name+'_'+str(i))}.wav\"\n            save_wav(wav_path, arr, sr)\n\n            item = {\"audio_filepath\": str(wav_path), \"text\": text, \"duration\": dur}\n            fo.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n            kept += 1\n\n    print(f\"[OK] {name}: {kept} записей\")\n    return {\"name\": name, \"manifest\": str(manifest), \"dir\": str(out_dir), \"kept\": kept}\n"}, {"cell_type": "markdown", "metadata": {}, "source": "# Подготовка датасетов (WAV + JSONL)\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "import os\nHF_TOKEN = os.environ.get(\"HF_TOKEN\") or os.environ.get(\"HUGGINGFACE_HUB_TOKEN\")\n\nprepared = []\nfor repo in LINKS:\n    for split in SPLITS:\n        meta = prepare_hf_dataset_to_wav(repo, split, DATA_DIR, RUPAT, hf_token=HF_TOKEN)\n        if meta and meta[\"kept\"] > 0:\n            prepared.append(meta)\n\nprint(\"Prepared:\", len(prepared), \"splits\")\nfor m in prepared[:5]:\n    print(\" -\", m[\"name\"], \"→\", m[\"kept\"])\n"}, {"cell_type": "markdown", "metadata": {}, "source": "# Инференс Canary + фильтрация + экспорт core/hard\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "import json, pandas as pd\nfrom jiwer import wer, cer\nfrom pathlib import Path\nfrom nemo.collections.asr.models import ASRModel\n\ndef load_canary(model_id: str, nemo_path: str | None):\n    if nemo_path:\n        return ASRModel.restore_from(nemo_path, map_location=\"cuda\").eval()\n    return ASRModel.from_pretrained(model_name=model_id).eval()\n\nmodel = load_canary(MODEL_ID, NEMO_PATH)\n\ndef transcribe_paths(model, paths, batch_size, source_lang, target_lang, task, pnc):\n    results = {}\n    bs = max(1, int(batch_size))\n    import torch, gc\n    for i in range(0, len(paths), bs):\n        batch = paths[i:i+bs]\n        hyps = model.transcribe(batch, batch_size=bs,\n                                source_lang=source_lang, target_lang=target_lang,\n                                task=task, pnc=pnc)\n        for p, h in zip(batch, hyps):\n            if isinstance(h, str): results[p] = h\n            elif isinstance(h, dict): results[p] = h.get(\"text\") or h.get(\"pred_text\") or str(h)\n            else: results[p] = str(h)\n        try:\n            torch.cuda.empty_cache(); torch.cuda.ipc_collect()\n        except Exception:\n            pass\n        gc.collect()\n    return results\n\ndef compute_metrics(items, preds):\n    out = []\n    for it in items:\n        a = it[\"audio_filepath\"]; ref = it[\"text\"]; d = it[\"duration\"]\n        hyp = preds.get(a, \"\")\n        if not hyp: \n            continue\n        out.append({\n            \"audio\": a, \"ref\": ref, \"hyp\": hyp, \"dur\": float(d),\n            \"wer\": float(wer(ref, hyp)), \"cer\": float(cer(ref, hyp))\n        })\n    return out\n\ndef split_by_quantiles(items, q_core, q_hard):\n    if not items: \n        return [], [], {\"q_core_val\":0.0,\"q_hard_val\":0.0}\n    s = pd.Series([it[\"wer\"] for it in items], dtype=float)\n    q_core_val = float(s.quantile(q_core)); q_hard_val = float(s.quantile(q_hard))\n    core = [it for it in items if it[\"wer\"] <= q_core_val]\n    hard = [it for it in items if (it[\"wer\"] > q_core_val) and (it[\"wer\"] <= q_hard_val)]\n    return core, hard, {\"q_core_val\": q_core_val, \"q_hard_val\": q_hard_val}\n\ndef export_bucket(bucket_name: str, items, ds_dir: Path):\n    out_dir = ds_dir / bucket_name\n    audio_dir = out_dir / \"audio\"\n    out_dir.mkdir(parents=True, exist_ok=True); audio_dir.mkdir(parents=True, exist_ok=True)\n    manifest = out_dir / \"manifest.jsonl\"\n    kept = 0\n    with manifest.open(\"w\", encoding=\"utf-8\") as fo:\n        for it in items:\n            row = {\"audio_filepath\": it[\"audio\"], \"text\": it[\"ref\"], \"pred_text\": it[\"hyp\"],\n                   \"wer\": it[\"wer\"], \"cer\": it[\"cer\"], \"duration\": it[\"dur\"]}\n            fo.write(json.dumps(row, ensure_ascii=False) + \"\\n\"); kept += 1\n    return kept, manifest\n\nsummaries = []\n\nfor meta in prepared:\n    man_path = Path(meta[\"manifest\"])\n    ds_dir   = Path(meta[\"dir\"])\n    with man_path.open(\"r\", encoding=\"utf-8\") as f:\n        items = [json.loads(x) for x in f]\n\n    uniq_paths = list({it[\"audio_filepath\"] for it in items})\n    preds = transcribe_paths(model, uniq_paths, BATCH_SIZE, SOURCE_LANG, TARGET_LANG, TASK, USE_PNC)\n\n    metrics_all = compute_metrics(items, preds)\n    pool = [m for m in metrics_all if (m[\"cer\"] <= CER_MAX and m[\"wer\"] <= WER_MAX)]\n    core_items, hard_items, qvals = split_by_quantiles(pool, Q_CORE, Q_HARD)\n\n    hard_kept, hard_manifest = export_bucket(\"hard\", hard_items, ds_dir)\n    core_kept, core_manifest = export_bucket(\"core\", core_items, ds_dir)\n\n    summary = {\n        \"dataset\": meta[\"name\"],\n        \"total\": len(items), \"pool\": len(pool),\n        \"q_core\": Q_CORE, \"q_hard\": Q_HARD,\n        \"q_core_val\": qvals[\"q_core_val\"], \"q_hard_val\": qvals[\"q_hard_val\"],\n        \"core_selected\": len(core_items), \"core_saved\": core_kept, \"core_manifest\": str(core_manifest),\n        \"hard_selected\": len(hard_items), \"hard_saved\": hard_kept, \"hard_manifest\": str(hard_manifest),\n        \"params\": {\"min_dur\": MIN_DUR, \"max_dur\": MAX_DUR, \"cer_max\": CER_MAX, \"wer_max\": WER_MAX,\n                   \"task\": TASK, \"pnc\": USE_PNC, \"source_lang\": SOURCE_LANG, \"target_lang\": TARGET_LANG},\n    }\n    with (ds_dir / \"summary.json\").open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(summary, f, ensure_ascii=False, indent=2)\n    summaries.append(summary)\n\nimport pandas as pd\ndf = pd.DataFrame(summaries)\ndf\n"}]}